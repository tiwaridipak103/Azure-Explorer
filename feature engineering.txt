import multiprocessing as mp
import numpy as np
import random
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split, KFold
import lightgbm as lgb

class RandomSearchParallel:
    def __init__(self, model, param_grid, scoring, n_iter=20, n_jobs=4, cv=5):
        self.model = model
        self.param_grid = param_grid
        self.scoring = scoring
        self.n_iter = n_iter  # Total number of random parameter sets to test
        self.n_jobs = min(n_jobs, mp.cpu_count())  # Limit parallel jobs to CPU cores
        self.cv = cv  # Number of cross-validation folds
        self.best_params_ = None
        self.best_score_ = float('inf')
        self.best_model_ = None

    def _train_evaluate(self, params, X, y, result_dict):
        """Trains and evaluates the model using cross-validation"""
        process_name = mp.current_process().name
        print(f"{process_name} - Training with params: {params}")

        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)
        scores = []

        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model = self.model.set_params(**params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            score = mean_absolute_error(y_val, y_pred)
            scores.append(score)

        mean_score = np.mean(scores)
        print(f"{process_name} - Mean CV Score: {mean_score}")

        # Store the result in shared dictionary
        result_dict[str(params)] = mean_score

    def fit(self, X, y):
        """Runs Randomized Search with batch-wise multiprocessing"""
        manager = mp.Manager()
        result_dict = manager.dict()  # Shared dict for storing results
        processes = []

        sampled_params = [self._sample_params() for _ in range(self.n_iter)]  # Sample random parameters

        for i, params in enumerate(sampled_params):
            p = mp.Process(target=self._train_evaluate, args=(params, X, y, result_dict))
            p.start()
            processes.append(p)

            # Run in batches of `n_jobs`
            if len(processes) >= self.n_jobs:
                for p in processes:
                    p.join()
                processes = []  # Reset batch

        # Ensure any remaining processes complete
        for p in processes:
            p.join()

        # Find the best parameters based on the lowest score
        self.best_params_, self.best_score_ = min(result_dict.items(), key=lambda x: x[1])
        self.best_model_ = self.model.set_params(**eval(self.best_params_))  # Train best model

        print(f"\nBest Params: {self.best_params_}")
        print(f"Best Score: {self.best_score_}")

    def _sample_params(self):
        """Randomly samples a set of hyperparameters from the grid"""
        return {key: random.choice(values) for key, values in self.param_grid.items()}


# Example Usage:
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    
    # Generate synthetic data
    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    
    # Define parameter grid
    param_grid = {
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7]
    }

    # Instantiate model
    model = lgb.LGBMRegressor()

    # Run Random Search
    search = RandomSearchParallel(model, param_grid, scoring="neg_mean_absolute_error", n_iter=20, n_jobs=4, cv=5)
    search.fit(X, y)

    print(f"Best Model: {search.best_model_}")


import psutil
import time

# Wait a short time to get an accurate reading
time.sleep(1) 

# Get the percentage usage of each core
cpu_usage_per_core = psutil.cpu_percent(percpu=True)

# Print the usage of each core
for i, usage in enumerate(cpu_usage_per_core):
    print(f"Core {i}: {usage}% used")


import psutil
import matplotlib.pyplot as plt
import time

plt.ion()  # Enable interactive mode

# Initialize the plot
fig, ax = plt.subplots()
bars = ax.bar(range(psutil.cpu_count()), [0] * psutil.cpu_count(), color='blue')

ax.set_ylim(0, 100)
ax.set_xlabel("CPU Core")
ax.set_ylabel("Usage (%)")
ax.set_title("Live CPU Usage per Core")

while True:
    # Get CPU usage for each core
    cpu_usage = psutil.cpu_percent(percpu=True)
    
    # Update bar heights
    for bar, usage in zip(bars, cpu_usage):
        bar.set_height(usage)
    
    # Pause to refresh the plot
    plt.pause(2)