import numpy as np
import multiprocessing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate random data
def generate_data(seed):
    np.random.seed(seed)
    X = np.random.rand(100, 5)  # 100 samples, 5 features
    y = X @ np.array([2, -3, 1, 4, -2]) + np.random.randn(100)  # Linear relation with noise
    return X, y

# Function to train a linear regression model
def train_linear_regression(seed):
    X, y = generate_data(seed)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    
    return f"Model {seed}: MSE = {mse:.4f}"

# Running 5 linear regression models in parallel
if __name__ == "__main__":
    seeds = [42, 43, 44, 45, 46]  # Different seeds for different datasets
    
    with multiprocessing.Pool(processes=5) as pool:
        results = pool.map(train_linear_regression, seeds)
    
    for res in results:
        print(res)



import multiprocessing as mp

def dipak(c, shared_list):
    shared_list.extend(c)
    print('hi')

if __name__ == "__main__":
    run_with_multiprocessing = True

    if run_with_multiprocessing:
        with mp.Manager() as manager:
            a = manager.list()  # Shared list across processes
            with mp.Pool(processes=3) as pool:
                pool.starmap(dipak, [((1, 2, 3), a)])

            print(list(a))  # Convert shared list to normal list before printing


import multiprocessing as mp

def dipak(c, shared_dict):
    shared_dict.update(c)  # Update shared dictionary
    print('hi')

if __name__ == "__main__":
    run_with_multiprocessing = True

    if run_with_multiprocessing:
        with mp.Manager() as manager:
            a = manager.dict()  # Shared dictionary across processes
            data = [{"x": 1, "y": 2}, {"z": 3}]

            with mp.Pool(processes=3) as pool:
                pool.starmap(dipak, [(d, a) for d in data])

            print(dict(a))  # Convert shared dict to normal dict before printing


import multiprocessing as mp

def dipak(c, shared_list, shared_dict):
    shared_list.extend(c['list'])  # Append values to shared list
    shared_dict.update(c['dict'])  # Update shared dictionary
    print('hi')

if __name__ == "__main__":
    run_with_multiprocessing = True

    if run_with_multiprocessing:
        with mp.Manager() as manager:
            shared_list = manager.list()  # Shared list across processes
            shared_dict = manager.dict()  # Shared dictionary across processes

            data = [
                {"list": [1, 2], "dict": {"a": 1}}, 
                {"list": [3, 4], "dict": {"b": 2}}
            ]

            with mp.Pool(processes=3) as pool:
                pool.starmap(dipak, [(d, shared_list, shared_dict) for d in data])

            print("Final List:", list(shared_list))  # Convert to normal list before printing
            print("Final Dict:", dict(shared_dict))  # Convert to normal dict before printing



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from multiprocessing import Pool

# Sample dataset
np.random.seed(42)
data = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.rand(100),
    'target': np.random.randint(0, 2, 100)
})

# Function to train and evaluate a model
def dipak_model(seed):
    # Shuffle data
    np.random.seed(seed)
    shuffled_data = data.sample(frac=1, random_state=seed).reset_index(drop=True)
    
    # Train-test split (80-20)
    train, test = train_test_split(shuffled_data, test_size=0.2, random_state=seed)

    # Data preprocessing (if any)
    X_train = train[['feature1', 'feature2']]
    y_train = train['target']
    X_test = test[['feature1', 'feature2']]
    y_test = test['target']
    
    # Train a Linear Regression model
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Evaluate the model
    score = model.score(X_test, y_test)
    
    return f"Seed {seed}: Model Score = {score:.4f}"

# Running 10 parallel executions using multiprocessing
if __name__ == "__main__":
    seeds = list(range(10))  # 10 different seeds for different splits
    with Pool(processes=4) as pool:  # Using 4 cores
        results = pool.map(dipak_model, seeds)

    # Print results
    for res in results:
        print(res)




import multiprocessing as mp
import numpy as np
import random
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import lightgbm as lgb

class RandomSearchParallel:
    def __init__(self, model, param_grid, scoring, n_iter=10, n_jobs=4):
        self.model = model
        self.param_grid = param_grid
        self.scoring = scoring
        self.n_iter = n_iter
        self.n_jobs = min(n_jobs, mp.cpu_count() - 1)  # Limit to available cores
        self.best_params_ = None
        self.best_score_ = float('inf')
        self.best_model_ = None

    def _train_evaluate(self, params, X_train, X_test, y_train, y_test, result_dict):
        """Trains and evaluates the model for given hyperparameters"""
        process_name = mp.current_process().name
        print(f"{process_name} - Training with params: {params}")

        # Initialize model with current hyperparameters
        model = self.model.set_params(**params)

        # Train the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate the score
        score = mean_absolute_error(y_test, y_pred)
        print(f"{process_name} - Score: {score}")

        # Store the result in shared dictionary
        result_dict[str(params)] = score

    def fit(self, X, y):
        """Runs Randomized Search in parallel"""
        manager = mp.Manager()
        result_dict = manager.dict()  # Shared dict for storing results
        processes = []

        # Randomly sample `n_iter` parameter sets
        sampled_params = [self._sample_params() for _ in range(self.n_iter)]

        # Split dataset
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Launch processes
        for i, params in enumerate(sampled_params):
            p = mp.Process(target=self._train_evaluate, args=(params, X_train, X_test, y_train, y_test, result_dict))
            p.start()
            processes.append(p)

            # Limit parallel jobs to `n_jobs`
            if len(processes) >= self.n_jobs:
                for p in processes:
                    p.join()
                processes = []

        # Ensure all processes finish
        for p in processes:
            p.join()

        # Find the best parameters
        self.best_params_, self.best_score_ = min(result_dict.items(), key=lambda x: x[1])
        self.best_model_ = self.model.set_params(**eval(self.best_params_))  # Train best model

        print(f"\nBest Params: {self.best_params_}")
        print(f"Best Score: {self.best_score_}")

    def _sample_params(self):
        """Randomly samples a set of hyperparameters from the grid"""
        return {key: random.choice(values) for key, values in self.param_grid.items()}


# Sample dataset
from sklearn.datasets import make_regression
X, y = make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)

# Define hyperparameter grid
param_grid = {
    'num_leaves': [20, 31, 40, 50],
    'max_depth': [-1, 5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
}

# Initialize and run Randomized Search
search = RandomSearchParallel(model=lgb.LGBMRegressor(), param_grid=param_grid, scoring="neg_mean_absolute_error", n_iter=8, n_jobs=4)
search.fit(X, y)

from multiprocessing import Manager, Process

def computation(iter, scenerio_df, response, important_feature, all_models, properties, val, all_predict):
    # Your computation logic here
    pass

iteration = 10  # Define your iteration count

with Manager() as manager:
    all_predict = manager.dict()

    processes = [Process(target=computation, args=(i, scenerio_df, response, important_feature, all_models, properties, 1, all_predict)) for i in range(iteration)]
    
    [p.start() for p in processes], [p.join() for p in processes]  # Start and join processes in one line

    scenerio_df_output.update({f'PREDICTION_{i + 1}': dict(all_predict)[f'PREDICTION_{i + 1}'][0] for i in range(iteration)})  # Update DataFrame in one line