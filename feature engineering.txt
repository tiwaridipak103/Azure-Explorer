import numpy as np
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from lightgbm import LGBMRegressor

# Define Custom Weighted MAE Loss for Training
def weighted_mae_obj(y_pred, dataset):
    y_true = dataset.get_label()
    var_y = np.var(y_true)
    weight = 1 / (var_y + 1e-6)  # Prevent division by zero

    grad = weight * np.sign(y_pred - y_true)
    hess = np.ones_like(y_true)  # MAE has no second derivative

    return grad, hess

# Define Custom Weighted MAE Metric for Evaluation
def weighted_mae_metric(y_pred, dataset):
    y_true = dataset.get_label()
    var_y = np.var(y_true)
    weight = 1 / (var_y + 1e-6)

    mae = np.mean(np.abs(y_pred - y_true))
    weighted_mae = weight * mae  

    return "weighted_mae", weighted_mae, False  # False: lower is better

# Define Model
lgb_model = LGBMRegressor(n_jobs=-1)

# Define Hyperparameter Grid
param_grid = {
    'num_leaves': [20, 40, 60],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 300, 500],
    'max_depth': [-1, 10, 20],
}

# Perform Randomized Search
random_search = RandomizedSearchCV(
    estimator=lgb_model,
    param_distributions=param_grid,
    n_iter=10,
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=2,
    n_jobs=-1
)

# Fit Model with Custom Objective and Metric
random_search.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    eval_metric=weighted_mae_metric,  # Custom evaluation metric
    callbacks=[lgb.log_evaluation(50)],
    objective=weighted_mae_obj  # Custom objective function
)