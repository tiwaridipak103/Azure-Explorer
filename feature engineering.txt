import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Generator Model
def make_generator_model(input_dim, feature_dim):
    model = tf.keras.Sequential()
    model.add(layers.Dense(128, activation="relu", input_dim=input_dim))
    model.add(layers.Reshape((1, 128)))
    model.add(layers.LSTM(256, return_sequences=True))
    model.add(layers.LSTM(128, return_sequences=False))
    model.add(layers.Dense(feature_dim, activation="tanh"))
    return model

# Discriminator Model
def make_discriminator_model(feature_dim):
    model = tf.keras.Sequential()
    model.add(layers.LSTM(128, return_sequences=True, input_shape=(1, feature_dim)))
    model.add(layers.LSTM(64, return_sequences=False))
    model.add(layers.Dense(1, activation="sigmoid"))
    return model

# Loss functions
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss

# Optimizers
generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Parameters
BUFFER_SIZE = 1000
BATCH_SIZE = 32
EPOCHS = 100
noise_dim = 100
feature_dim = 50  # Number of features in each row
num_examples_to_generate = 5

# Sample Data (Replace this with your time-series data)
data = np.random.randn(1000, feature_dim).astype(np.float32)
train_dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# Models
generator = make_generator_model(noise_dim, feature_dim)
discriminator = make_discriminator_model(feature_dim)

# Generate random vectors for testing
seed = tf.random.normal([num_examples_to_generate, noise_dim])

@tf.function
def train_step(real_data):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_data = generator(noise, training=True)

        real_output = discriminator(tf.expand_dims(real_data, axis=1), training=True)
        fake_output = discriminator(tf.expand_dims(generated_data, axis=1), training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

def train(dataset, epochs):
    for epoch in range(epochs):
        gen_loss_avg = 0
        disc_loss_avg = 0

        for batch in dataset:
            gen_loss, disc_loss = train_step(batch)
            gen_loss_avg += gen_loss
            disc_loss_avg += disc_loss

        print(f"Epoch {epoch+1}/{epochs}, Generator Loss: {gen_loss_avg:.4f}, Discriminator Loss: {disc_loss_avg:.4f}")
        if (epoch + 1) % 10 == 0:
            generate_and_save_data(generator, epoch + 1, seed)

def generate_and_save_data(model, epoch, test_input):
    predictions = model(test_input, training=False)
    print(f"Generated data at epoch {epoch}:\n{predictions.numpy()}")

# Train the GAN
train(train_dataset, EPOCHS)



from sklearn.base import BaseEstimator, RegressorMixin
from statsmodels.regression.mixed_linear_model import MixedLM
import numpy as np
import pandas as pd
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Custom wrapper for statsmodels MixedLM
class MixedLMWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, formula, groups):
        self.formula = formula
        self.groups = groups
        self.model = None

    def fit(self, X, y):
        # Convert X and y into a DataFrame for statsmodels
        data = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(X.shape[1])])
        data["target"] = y
        data["groups"] = self.groups
        
        # Fit the MixedLM model
        self.model = MixedLM.from_formula(self.formula, data, groups=data["groups"])
        self.result = self.model.fit()
        return self

    def predict(self, X):
        # Use the fitted model to predict
        if self.model is None:
            raise ValueError("Model must be fitted before calling predict.")
        
        # Convert X into a DataFrame
        data = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(X.shape[1])])
        return self.result.predict(data)

# Example dataset
np.random.seed(0)
X = np.random.randn(100, 2)
y = 1.5 * X[:, 0] - 2.0 * X[:, 1] + np.random.randn(100) * 0.1
groups = np.random.choice(['A', 'B', 'C', 'D'], size=100)

# Train-test split
X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(X, y, groups, test_size=0.2, random_state=42)

# Define MixedLM formula
formula = "target ~ feature_0 + feature_1"

# Define base models
mixedlm_model = MixedLMWrapper(formula=formula, groups=groups_train)
linear_model = LinearRegression()

# Define stacking regressor
stacking_regressor = StackingRegressor(
    estimators=[
        ('mixedlm', mixedlm_model),
        ('linear', linear_model)
    ],
    final_estimator=LinearRegression()
)

# Train stacking model
stacking_regressor.fit(X_train, y_train)

# Make predictions
y_pred = stacking_regressor.predict(X_test)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

