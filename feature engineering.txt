import psutil
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import time
import os
from IPython.display import display, clear_output

def set_cpu_affinity(core_id):
    process = psutil.Process(os.getpid())
    process.cpu_affinity([core_id])

def update_plot():
    set_cpu_affinity(45)
    sns.set_style("darkgrid")
    plt.ion()
    fig, ax = plt.subplots(figsize=(25, 6))
    num_cores = psutil.cpu_count()
    bars = ax.bar(range(num_cores), [0] * num_cores, color='blue', alpha=0.7)
    ax.set_ylim(0, 110)
    ax.set_xlabel("CPU Core", fontsize=12)
    ax.set_ylabel("Usage (%)", fontsize=12)
    ax.set_title("Live CPU Usage per Core", fontsize=14, fontweight='bold', pad=50)
    ax.yaxis.grid(True, linestyle="-", alpha=0.7)
    text_labels = [ax.text(bar.get_x() + bar.get_width() / 2, 0, '', ha='center', va='bottom', fontsize=10, fontweight='bold') for bar in bars]

    while True:
        cpu_usage = psutil.cpu_percent(percpu=True)
        for bar, usage, text in zip(bars, cpu_usage, text_labels):
            bar.set_height(usage)
            if usage <= 40:
                bar.set_color("green")
            elif 40 < usage <= 80:
                bar.set_color("orange")
            else:
                bar.set_color("red")
            text.set_y(usage + 2)
            text.set_text(f"{usage:.1f}%")
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(1)
        clear_output(wait=True)

plt.show()
update_plot()


import os
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr
import pandas as pd

# Import necessary R packages
fpp3 = importr("fpp3")
tsibbledata = importr("tsibbledata")
tsibble = importr("tsibble")
fma = importr("fma")

# Function to export dataset to CSV
def ts2csv(objname, pname):
    try:
        # Get the dataset
        robj = robjects.r[f"data('{objname}')"]
        df = robjects.r[objname]  # Convert R dataset to Python-readable format
        df = pd.DataFrame({col: list(df.rx2(col)) for col in df.names})

        # Create directory if not exists
        dir_path = f"data/{pname}"
        os.makedirs(dir_path, exist_ok=True)
        
        # Save to CSV
        fname = f"{dir_path}/{objname}.csv"
        df.to_csv(fname, index=False)
        print(f"Saved: {fname}")
    
    except Exception as e:
        print(f"Error saving {objname}: {str(e)}")

# Loop through all packages and datasets
for pname in ["fpp3", "tsibbledata", "tsibble", "fma"]:
    try:
        # Get available datasets in the package
        datasets = robjects.r(f"data(package='{pname}')")  
        dataset_names = datasets[3]  # Extract dataset names

        # Convert each dataset to CSV
        for objname in dataset_names:
            ts2csv(objname, pname)
    
    except Exception as e:
        print(f"Error loading package {pname}: {str(e)}")


import os
import requests

# GitHub repo containing the exact R datasets
base_url = "https://raw.githubusercontent.com/asif-mehedi/fpp3-data/main/data/"
datasets = [
    "aus_airpassengers.csv", "aus_arrivals.csv", "aus_cafe.csv", "aus_livestock.csv",
    "aus_production.csv", "bricksq.csv", "electricity.csv", "tourism.csv"
]  # Add more datasets if needed

# Create a directory for storing the datasets
os.makedirs("fpp3_data", exist_ok=True)

# Download and save each dataset
for dataset in datasets:
    url = base_url + dataset
    response = requests.get(url)

    if response.status_code == 200:
        with open(f"fpp3_data/{dataset}", "wb") as file:
            file.write(response.content)
        print(f"Downloaded: {dataset}")
    else:
        print(f"Failed to download: {dataset}")

print("All datasets downloaded successfully!")


import os
import requests

# GitHub repository details
repo_owner = "asif-mehedi"
repo_name = "fpp3-data"
folder_path = "data"  # Change this to the folder you want to download
base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}"

# Local save path
save_dir = os.path.join(os.getcwd(), folder_path)
os.makedirs(save_dir, exist_ok=True)

# Fetch file list
response = requests.get(base_url)
if response.status_code == 200:
    files = response.json()
    
    for file in files:
        if file["type"] == "file":  # Ensure it's a file, not a subfolder
            file_url = file["download_url"]
            file_name = os.path.join(save_dir, file["name"])

            # Download the file
            file_data = requests.get(file_url)
            with open(file_name, "wb") as f:
                f.write(file_data.content)
            print(f"Downloaded: {file_name}")
else:
    print(f"Failed to fetch folder contents. Status code: {response.status_code}")



import pandas as pd

# Read the copied text from the file
with open("data.txt", "r", encoding="utf-8") as file:
    csv_text = file.read()

# Convert the text into a list of lines
lines = csv_text.strip().split("\n")

# Convert into a CSV file
with open("output.csv", "w", encoding="utf-8") as f:
    for line in lines:
        f.write(line + "\n")

print("CSV file created successfully as output.csv")


import numpy as np
import pandas as pd

# Assuming model is already trained
feature_importance = model.feature_importances_
feature_names = model.feature_name_

# Convert to DataFrame
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})

# Min-Max Scaling to range [0, 100]
min_val = feature_importance_df['Importance'].min()
max_val = feature_importance_df['Importance'].max()

feature_importance_df['Scaled Importance'] = (feature_importance_df['Importance'] - min_val) / (max_val - min_val) * 100

# Sort in descending order
feature_importance_df = feature_importance_df.sort_values(by="Scaled Importance", ascending=False)

print(feature_importance_df)


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Function to reverse time series
def reverse_ts(y):
    return pd.Series(y[::-1], index=y.index[::-1])

# Function to reverse a forecast
def reverse_forecast(forecast_result, original_series):
    h = len(forecast_result.predicted_mean)
    reversed_x = reverse_ts(original_series)
    reversed_mean = reverse_ts(forecast_result.predicted_mean)
    reversed_conf_int = forecast_result.conf_int().iloc[::-1]
    
    return {
        "x": reversed_x,
        "mean": reversed_mean,
        "lower": reversed_conf_int.iloc[:, 0],
        "upper": reversed_conf_int.iloc[:, 1]
    }

# Example dataset (replace with euretail dataset)
dates = pd.date_range(start='1996-01-01', periods=60, freq='Q')  # 15 years of quarterly data
euretail = pd.Series(np.random.randn(60).cumsum(), index=dates)

# Backcasting process
reversed_series = reverse_ts(euretail)
model = ARIMA(reversed_series, order=(1,1,1))
fit = model.fit()
forecast_result = fit.get_forecast(steps=8)
backcast = reverse_forecast(forecast_result, euretail)

# Plot the backcast
plt.figure(figsize=(10, 5))
plt.plot(euretail.index, euretail, label='Original Series', color='blue')
plt.plot(euretail.index[:8], backcast["mean"], label='Backcast', color='red', linestyle='dashed')
plt.fill_between(euretail.index[:8], backcast["lower"], backcast["upper"], color='red', alpha=0.3)
plt.title('Backcasts for Quarterly Retail Trade in the Euro Area')
plt.legend()
plt.show()
