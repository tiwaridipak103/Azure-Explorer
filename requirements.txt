TTimport numpy as np
import pandas as pd
from scipy.stats import ttest_ind, mannwhitneyu, ks_2samp, levene, shapiro

# Generate Sample Data
np.random.seed(42)
df1 = pd.DataFrame(np.random.normal(50, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])
df2 = pd.DataFrame(np.random.normal(50, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])

# Function to Perform Hypothesis Testing
def compare_dataframes(df1, df2, alpha=0.05):
    results = []

    for col in df1.columns:
        data1, data2 = df1[col], df2[col]

        # Normality Check (Shapiro-Wilk Test)
        norm1 = shapiro(data1).pvalue > alpha
        norm2 = shapiro(data2).pvalue > alpha

        # T-Test if normal, otherwise Mann-Whitney U Test
        if norm1 and norm2:
            test_stat, p_value_ttest = ttest_ind(data1, data2)
            test_used = "T-Test"
        else:
            test_stat, p_value_ttest = mannwhitneyu(data1, data2)
            test_used = "Mann-Whitney U"

        # Variance Test (Leveneâ€™s Test)
        _, p_value_levene = levene(data1, data2)

        # KS Test for Distribution Similarity
        _, p_value_ks = ks_2samp(data1, data2)

        # Store Results
        results.append({
            "Column": col,
            "Test Used": test_used,
            "T/Mann-Whitney P-Value": p_value_ttest,
            "Levene P-Value (Variance)": p_value_levene,
            "KS P-Value (Distribution)": p_value_ks,
            "Same Mean?": p_value_ttest > alpha,
            "Same Variance?": p_value_levene > alpha,
            "Same Distribution?": p_value_ks > alpha
        })

    return pd.DataFrame(results)

# Run the comparison
results_df = compare_dataframes(df1, df2)
print(results_df)



import numpy as np
import pandas as pd

def permutation_test(df1, df2, num_permutations=10000, alpha=0.05):
    results = []
    
    for col in df1.columns:
        data1, data2 = df1[col].values, df2[col].values
        observed_diff = np.mean(data1) - np.mean(data2)  # Step 1: Observed difference

        # Step 2: Compute permuted mean differences
        combined = np.concatenate([data1, data2])
        perm_diffs = []
        for _ in range(num_permutations):
            np.random.shuffle(combined)  # Shuffle combined data
            perm_diffs.append(np.mean(combined[:len(data1)]) - np.mean(combined[len(data1):]))

        # Step 3: Compute empirical p-value
        perm_diffs = np.array(perm_diffs)
        p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))

        # Store results
        results.append({
            "Column": col,
            "Observed Difference": observed_diff,
            "P-Value": p_value,
            "Significant?": p_value < alpha  # Reject null if p < alpha
        })

    return pd.DataFrame(results)

# Example Usage
np.random.seed(42)
df1 = pd.DataFrame(np.random.normal(50, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])
df2 = pd.DataFrame(np.random.normal(51, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])  # Slightly different mean

# Run the permutation test
perm_results = permutation_test(df1, df2)
print(perm_results)


import numpy as np
import pandas as pd
from scipy.stats import f_oneway

def anova_test(df1, df2, alpha=0.05):
    results = []
    
    for col in df1.columns:
        data1, data2 = df1[col], df2[col]
        
        # Perform One-Way ANOVA
        f_stat, p_value = f_oneway(data1, data2)

        # Store results
        results.append({
            "Column": col,
            "F-Statistic": f_stat,
            "P-Value": p_value,
            "Significant?": p_value < alpha  # Reject null if p < alpha
        })
    
    return pd.DataFrame(results)

# Example Usage
np.random.seed(42)
df1 = pd.DataFrame(np.random.normal(50, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])
df2 = pd.DataFrame(np.random.normal(51, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])  # Slightly different mean

anova_results = anova_test(df1, df2)
print(anova_results)


import numpy as np
import pandas as pd
from scipy.stats import pearsonr, spearmanr

# Generate synthetic data
np.random.seed(42)
df1 = pd.DataFrame(np.random.normal(50, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])
df2 = pd.DataFrame(np.random.normal(51, 10, (100, 20)), columns=[f"col_{i}" for i in range(20)])

def correlation_tests(df1, df2):
    results = []
    for col in df1.columns:
        pearson_corr, _ = pearsonr(df1[col], df2[col])
        spearman_corr, _ = spearmanr(df1[col], df2[col])
        results.append({"Column": col, "Pearson Correlation": pearson_corr, "Spearman Correlation": spearman_corr})
    
    return pd.DataFrame(results)

# Run tests
corr_results = correlation_tests(df1, df2)
print(corr_results)
from scipy.stats import entropy

def kl_divergence(df1, df2, bins=20):
    results = []
    for col in df1.columns:
        hist1, _ = np.histogram(df1[col], bins=bins, density=True)
        hist2, _ = np.histogram(df2[col], bins=bins, density=True)

        # Normalize histograms to form probability distributions
        hist1 = hist1 / np.sum(hist1)
        hist2 = hist2 / np.sum(hist2)

        kl_div = entropy(hist1, hist2)  # KL divergence
        results.append({"Column": col, "KL Divergence": kl_div})
    
    return pd.DataFrame(results)

# Run KL test
kl_results = kl_divergence(df1, df2)
print(kl_results)



{
  "editor.formatOnSave": true,
  "editor.suggestSelection": "first",
  "files.autoSave": "afterDelay",
  "files.exclude": {
    "**/.git": true,
    "**/.svn": true,
    "**/.hg": true,
    "**/CVS": true,
    "**/.DS_Store": true,
    "**/*.pyc": {
      "when": "$(basename).py"
    },
    "**/_pycache_": true,
    ".pytest_cache": true
  },
  "files.trimTrailingWhitespace": true,
  "workbench.startupEditor": "newUntitledFile",
  "win-ca.inject": "append",
  "workbench.colorTheme": "Visual Studio Light",
  "editor.fontSize": 17,
  "git.autofetch": true,
  "snowflake.connections ConfigFile": "C:\\Users\\t057831\\.snowflake\\connections.toml",
  "workbench.preferredHighContrastColorTheme": "Default Dark Modern",
  "editor.rename.enablePreview": false,
  "editor.semanticTokenColorCustomizations": {}
}


Custom Tooltip =
VAR ActualValue = SELECTEDVALUE(YourTable[Actual])
VAR UpperValue = SELECTEDVALUE(YourTable[Upper])
VAR LowerValue = SELECTEDVALUE(YourTable[Lower])
VAR PredictionValue = SELECTEDVALUE(YourTable[Prediction])

VAR DisplayUpper = IF(ActualValue = UpperValue, BLANK(), UpperValue)
VAR DisplayLower = IF(ActualValue = LowerValue, BLANK(), LowerValue)

RETURN 
"Actual: " & ActualValue & UNICHAR(10) & 
IF(NOT(ISBLANK(DisplayUpper)), "Upper: " & DisplayUpper & UNICHAR(10), "") & 
"Prediction: " & PredictionValue & UNICHAR(10) & 
IF(NOT(ISBLANK(DisplayLower)), "Lower: " & DisplayLower, "")



Investment Gain and Investment Income are related but distinct financial concepts:

1. Investment Gain refers to the increase in the value of an investment over time. It includes:

Capital Gains: The profit earned from selling an asset (e.g., stocks, real estate) for more than its purchase price.

Unrealized Gains: The increase in value of an asset that has not yet been sold.



2. Investment Income refers to the regular earnings generated by an investment. It includes:

Dividends: Payments made by companies to shareholders.

Interest: Earnings from fixed-income investments like bonds or savings accounts.

Rental Income: Earnings from real estate properties.




In summary, investment gain is typically associated with asset appreciation, while investment income comes from ongoing earnings like dividends and interest.

