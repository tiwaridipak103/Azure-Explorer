import os
import pandas as pd

# Folder path where your CSV files are stored
folder_path = '/path/to/csv/folder'

# Initialize lists to store the dataframes
hadoop_dfs = []
snowflake_dfs = []

# Loop through all the files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.csv'):
        # Determine if it's a Hadoop or Snowflake file based on the filename
        if 'hadoop' in filename.lower():
            df = pd.read_csv(os.path.join(folder_path, filename))
            df['source_table'] = filename  # Add filename to identify which table this row came from
            hadoop_dfs.append(df)
        elif 'snowflake' in filename.lower():
            df = pd.read_csv(os.path.join(folder_path, filename))
            df['source_table'] = filename  # Add filename to identify which table this row came from
            snowflake_dfs.append(df)

# Concatenate all the Hadoop and Snowflake DataFrames
hadoop_df = pd.concat(hadoop_dfs, ignore_index=True)
snowflake_df = pd.concat(snowflake_dfs, ignore_index=True)

# Perform a left join based on the 'id' column (you can specify any other common columns)
# Assuming 'id' is the common key
merged_df = hadoop_df.merge(snowflake_df, how='left', on='id', suffixes=('_hadoop', '_snowflake'))

# Compare columns from both tables and fill 'Yes' if they match, 'No' otherwise
# Loop through all common columns to compare
for column in hadoop_df.columns:
    if column in snowflake_df.columns and column != 'id':  # Exclude 'id' from comparison
        merged_df[f'{column}_comparison'] = merged_df[f'{column}_hadoop'] == merged_df[f'{column}_snowflake']
        merged_df[f'{column}_comparison'] = merged_df[f'{column}_comparison'].replace({True: 'Yes', False: 'No'})

# Drop the original columns (optional) to keep only the comparison results
comparison_columns = [col for col in merged_df.columns if '_comparison' in col]
result_df = merged_df[['id', 'source_table_hadoop', 'source_table_snowflake'] + comparison_columns]

# Output result
print(result_df.head())
