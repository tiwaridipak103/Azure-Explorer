Certainly! Below is a shell script (cleanup_old_dags.sh) that achieves the same functionality—deleting old DAG metadata from the PostgreSQL database. You can execute this script periodically (e.g., using cron or another scheduler) to remove old DAG run data based on the retention period.

Shell Script: cleanup_old_dags.sh
bash
Copy code
#!/bin/bash

# PostgreSQL connection details
DB_HOST="localhost"
DB_NAME="airflow"
DB_USER="airflow"
DB_PASS="your_password"
DB_PORT="5432"

# Set the retention period (in days)
RETENTION_DAYS=30

# Calculate the retention date (current date minus retention days)
RETENTION_DATE=$(date -d "-${RETENTION_DAYS} days" +"%Y-%m-%d %H:%M:%S")

# SQL queries to delete old records from the PostgreSQL database
DELETE_DAG_RUN="DELETE FROM dag_run WHERE execution_date < '$RETENTION_DATE';"
DELETE_TASK_INSTANCE="DELETE FROM task_instance WHERE execution_date < '$RETENTION_DATE';"
DELETE_LOG="DELETE FROM log WHERE dttm < '$RETENTION_DATE';"

# Log file for tracking the cleanup process
LOG_FILE=~/airflow_cleanup.log

# Function to run the cleanup queries
run_cleanup() {
    echo "[$(date)] Starting cleanup process..." | tee -a $LOG_FILE

    PGPASSWORD=$DB_PASS psql -h $DB_HOST -U $DB_USER -d $DB_NAME -p $DB_PORT -c "$DELETE_TASK_INSTANCE" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old task instances." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting task instances." | tee -a $LOG_FILE
    fi

    PGPASSWORD=$DB_PASS psql -h $DB_HOST -U $DB_USER -d $DB_NAME -p $DB_PORT -c "$DELETE_DAG_RUN" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old DAG runs." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting DAG runs." | tee -a $LOG_FILE
    fi

    PGPASSWORD=$DB_PASS psql -h $DB_HOST -U $DB_USER -d $DB_NAME -p $DB_PORT -c "$DELETE_LOG" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old logs." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting logs." | tee -a $LOG_FILE
    fi

    echo "[$(date)] Cleanup process completed." | tee -a $LOG_FILE
}

# Run the cleanup function
run_cleanup
Explanation of the Shell Script:
PostgreSQL Connection Details:

The connection details (DB_HOST, DB_NAME, DB_USER, DB_PASS, DB_PORT) are defined for the PostgreSQL instance where Airflow metadata is stored.
Retention Period:

The script calculates the RETENTION_DATE by subtracting the retention period (in days) from the current date. In this example, it's set to 30 days, but you can modify the RETENTION_DAYS variable.
SQL Queries:

There are three SQL queries to delete older records from the dag_run, task_instance, and log tables in the Airflow metadata database. The script passes the RETENTION_DATE as a cutoff point to only delete records older than this date.
Logging:

The script logs the cleanup process to ~/airflow_cleanup.log. You can change this path to your preferred log location.
psql Command:

The PostgreSQL command-line utility psql is used to execute the SQL queries. The password for the PostgreSQL user is passed using the PGPASSWORD environment variable.
Error Handling:

The script checks if each query was successful (if [ $? -eq 0 ]) and logs success or failure for each table deletion process.
How to Use the Shell Script:
Save the Script: Save the script as cleanup_old_dags.sh in your desired location.

Make it Executable: Make the script executable by running the following command:

bash
Copy code
chmod +x cleanup_old_dags.sh
Run the Script: Run the script manually or schedule it using cron to run at regular intervals (e.g., daily, weekly):

bash
Copy code
./cleanup_old_dags.sh
Schedule with Cron: To run the script daily at midnight, add the following entry to your crontab file:

bash
Copy code
0 0 * * * /path/to/cleanup_old_dags.sh
This script will automatically delete old DAG metadata from the PostgreSQL database based on the retention period you set. It logs the process, so you can review the actions taken by the script.






You said:
actually when i had created postgresql , i had given proid grant access 
like :
createrole -- login --- proid -- expiry -- infinity 

so instead of 
DB_USER="airflow"
DB_PASS="your_password"

how to incarporate proid access in shell script
ChatGPT said:
ChatGPT
When you are using a specific PostgreSQL role (proid) that has been granted certain privileges such as createrole, login, etc., you don’t need to set the DB_USER and DB_PASS if proid can log in without a password. You can use the psql command directly under the user context of proid (which avoids hardcoding a password).

Steps for Using proid with Shell Script:
Ensure proid Can Log In: If proid can log in without requiring a password (because it's set up for trust authentication or has a .pgpass file), you don't need to set DB_USER or DB_PASS.

Using proid in the Shell Script: You would switch to the proid user (or run the script as the proid user) and use the psql command without specifying DB_USER or DB_PASS in the shell script.

Example Shell Script for proid:
Below is an updated version of the script that assumes you're running it as the proid user (or the PostgreSQL instance is configured to allow proid to log in without a password).

bash
Copy code
#!/bin/bash

# PostgreSQL connection details
DB_HOST="localhost"
DB_NAME="airflow"
DB_PORT="5432"

# Set the retention period (in days)
RETENTION_DAYS=30

# Calculate the retention date (current date minus retention days)
RETENTION_DATE=$(date -d "-${RETENTION_DAYS} days" +"%Y-%m-%d %H:%M:%S")

# SQL queries to delete old records from the PostgreSQL database
DELETE_DAG_RUN="DELETE FROM dag_run WHERE execution_date < '$RETENTION_DATE';"
DELETE_TASK_INSTANCE="DELETE FROM task_instance WHERE execution_date < '$RETENTION_DATE';"
DELETE_LOG="DELETE FROM log WHERE dttm < '$RETENTION_DATE';"

# Log file for tracking the cleanup process
LOG_FILE=~/airflow_cleanup.log

# Function to run the cleanup queries
run_cleanup() {
    echo "[$(date)] Starting cleanup process..." | tee -a $LOG_FILE

    psql -h $DB_HOST -d $DB_NAME -p $DB_PORT -c "$DELETE_TASK_INSTANCE" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old task instances." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting task instances." | tee -a $LOG_FILE
    fi

    psql -h $DB_HOST -d $DB_NAME -p $DB_PORT -c "$DELETE_DAG_RUN" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old DAG runs." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting DAG runs." | tee -a $LOG_FILE
    fi

    psql -h $DB_HOST -d $DB_NAME -p $DB_PORT -c "$DELETE_LOG" 2>>$LOG_FILE
    if [ $? -eq 0 ]; then
        echo "[$(date)] Deleted old logs." | tee -a $LOG_FILE
    else
        echo "[$(date)] Error deleting logs." | tee -a $LOG_FILE
    fi

    echo "[$(date)] Cleanup process completed." | tee -a $LOG_FILE
}

# Run the cleanup function
run_cleanup
Key Points:
Running as proid: If you want to run this script under the proid user, you could either switch users before running it or run the script directly as proid:

bash
Copy code
sudo -u proid ./cleanup_old_dags.sh
Using .pgpass for Authentication: If proid requires a password to log in but you don't want to hardcode it in the script, you can set up a .pgpass file in the home directory of proid. This file stores the credentials securely and allows psql to log in without prompting for a password.

Example .pgpass file:
ruby
Copy code
localhost:5432:airflow:proid:your_password
The .pgpass file should be stored in the home directory of proid (~/.pgpass) and have permission set to 0600:

bash
Copy code
chmod 600 ~/.pgpass
With this setup, you don’t need to specify the DB_USER or DB_PASS in your script—psql will read the credentials from the .pgpass file if needed.
